{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c164c4-fe28-4d70-a21b-91349b98cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7f1706-7b29-410e-a53a-ac8e2e4f5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train dataset on MNIST :(\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3399d09c-3b39-44a8-977e-6c4cd9ec10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phd_school.dataset import PrintedMNIST, AddGaussianNoise, AddSPNoise  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a2c3d8-af6b-4347-9342-4346a214e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phd_school.models import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c94de9c-3241-4f69-b8d2-11fd5dce5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lr, batch_size, n_epochs, model):    \n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        # AddGaussianNoise(0, 1.0),\n",
    "        AddSPNoise(0.1),\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_set = PrintedMNIST(50000, -666, train_transform)\n",
    "    val_set = PrintedMNIST(5000, 33, val_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Get network\n",
    "    net = get_model(model)\n",
    "\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Running epoch {epoch+1}\")\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_correct = 0\n",
    "        total_processed = 0\n",
    "\n",
    "        for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "            net.train()\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute actual predictions\n",
    "            sm = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, y_hat = torch.max(sm, 1)\n",
    "\n",
    "            total_correct += (labels == y_hat.to(\"cpu\")).sum().item()\n",
    "            total_processed += len(labels)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 mini-batches\n",
    "                \n",
    "                running_loss = 0.0\n",
    "\n",
    "                # Check validation set\n",
    "                net.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    total_processed_val = 0\n",
    "                    total_correct_val = 0\n",
    "\n",
    "                    for data in val_loader:\n",
    "                        # get the inputs; data is a list of [inputs, labels]\n",
    "                        inputs, labels = data\n",
    "\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = net(inputs.to(device))\n",
    "                        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "                        # Compute actual predictions\n",
    "                        sm = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                        _, y_hat = torch.max(sm, 1)\n",
    "\n",
    "                        total_processed_val += len(labels)\n",
    "                        total_correct_val += (\n",
    "                            (labels == y_hat.to(\"cpu\")).sum().item()\n",
    "                        )\n",
    "\n",
    "                        running_loss += loss.item()\n",
    "\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        # Save model at the end of every epoch\n",
    "        torch.save(net.state_dict(), f\"../models/{model}_mnist.pth\")\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a113a89-6884-4f0b-89c0-c06053190af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Running epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████████| 196/196 [06:21<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "28%|██████████████████████▍                                                         | 55/196 [01:48<04:38,  1.97s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 74\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(lr, batch_size, n_epochs, model)\u001b[0m\n\u001b[0;32m     72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     73\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m---> 74\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Compute actual predictions\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\phd-school-PN9knR8Y-py3.11\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\phd-school-PN9knR8Y-py3.11\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\phd-school-PN9knR8Y-py3.11\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(1e-4, 256, 2, \"resnet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1cca5-2771-4cde-a602-7f471d8359be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
